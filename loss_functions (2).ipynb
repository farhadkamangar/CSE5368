{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTvNtRg9dq1R"
      },
      "source": [
        "# Loss Functions (Regression and Classification)\n",
        "\n",
        "A **loss function** measures how well a model's prediction matches the desired output.\n",
        "\n",
        "- **Regression:** the target is a real-valued quantity (e.g., house price).\n",
        "- **Classification:** the target is a class label (e.g., digit 0–9).\n",
        "\n",
        "This notebook defines common loss functions and walks through small **numerical examples**.\n",
        "\n",
        "*Farhad Kamangar — updated Feb. 2026\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqk4xyS2dq1S"
      },
      "source": [
        "## Regression losses\n",
        "### Mean Squared Error (MSE).\n",
        "\n",
        "Mean Squared Error is the most commonly used regression loss function and it is defined as the mean of the squared distances between the desired and the predicted output(s).\n",
        "\n",
        "$$\\large MSE=\\frac{1}{N}\\sum_{i=1}^{N} {(t_i-y_i)}^2$$\n",
        "\n",
        "where $N$ is the total number of samples , $t_i$ is the desired output for sample $i$ and $y_i$ is the actual output for sample $i$\n",
        "\n",
        "\n",
        "### Mean Absolute Error (MAE).\n",
        "\n",
        "Mean Absolute Error is another common function used for regression models and it is defined as as the mean of the absolute differences between the desired and the predicted output(s).\n",
        "\n",
        "$$\\large MAE=\\frac{1}{N}\\sum_{i=1}^{N} {|t_i-y_i|}$$\n",
        "\n",
        "where $N$ is the total number of samples , $t_i$ is the desired output for sample $i$ and $y_i$ is the actual output for sample $i$\n",
        "\n",
        "**Notes**\n",
        "- MSE penalizes large errors more strongly (squares the error).\n",
        "- MAE is more robust to outliers (linear penalty)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def mse(y, t):\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    t = np.asarray(t, dtype=float)\n",
        "    return np.mean((t - y) ** 2)\n",
        "\n",
        "def mae(y, t):\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    t = np.asarray(t, dtype=float)\n",
        "    return np.mean(np.abs(t - y))\n",
        "\n",
        "# --- Numerical example (regression) ---\n",
        "# Suppose a model predicts y for 5 samples and the desired outputs are t.\n",
        "y = np.array([2.0, 1.5, 0.0, 3.2, 4.1])\n",
        "t = np.array([1.0, 2.0, 0.0, 2.5, 5.0])\n",
        "\n",
        "print(\"y (pred):\", y)\n",
        "print(\"t (true):\", t)\n",
        "print(\"MSE:\", mse(y, t))\n",
        "print(\"MAE:\", mae(y, t))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swzWVAvopDiw",
        "outputId": "3acb63ce-ebc0-492b-c75f-818319012fe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y (pred): [2.  1.5 0.  3.2 4.1]\n",
            "t (true): [1.  2.  0.  2.5 5. ]\n",
            "MSE: 0.5100000000000001\n",
            "MAE: 0.6200000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Classification losses\n",
        "### Hinge (Multiclass Support Vector Machine) Loss\n",
        "\n",
        "The hinge loss function is used for classification and it is based on the concept of maximum-margin. The hinge loss for sample number $\\large s$ is formulated as:\n",
        "\n",
        "\n",
        "$$\\large L_s=\\sum_{j \\neq s_t}^{C} max(0,y_j-y_{s_t}+\\Delta)$$\n",
        "\n",
        "where $\\large s$ is the sample number, $\\large C$ is the number of classes, and  $\\large s_t$ is the index of the true class for sample number $\\large s$, and $\\Delta$ is a constant.\n",
        "\n",
        "\n",
        "The total loss across all the samples can be calculated as:\n",
        "\n",
        "$$\\large L=\\frac {1}{N} \\sum_{s} L_s$$\n",
        "\n",
        "\n",
        "where  $N$ is the number of the samples, $L$ is the total loss over all the samples and $s$ is the sample number\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FG5AWjCtknwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UpUAXgRAkfBe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negP-mRvdq1S"
      },
      "source": [
        "## Numerical Example\n",
        "Consider a one layer neural network with 3 nodes and linear activation function.\n",
        "This network is supposed to classify its input into one of three possible classes.\n",
        "Assume that the input to this network is 4 dimensional and the loss function is defined as SVM (Hinge).\n",
        "For simplicity assume that his network does not have any bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFcRwHAGdq1T",
        "outputId": "dceb4a05-f509-4e91-d5c5-b6b986877d6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X=  [[ 1.   1.   1.   3. ]\n",
            " [ 1.   0.   0.  -3.5]\n",
            " [ 0.   1.   0.   2. ]\n",
            " [ 5.   1.   2.   3. ]\n",
            " [ 3.   6.   5.   1. ]\n",
            " [ 3.   7.   7.   1. ]]\n",
            "weights:  [[2 4 7]\n",
            " [1 5 6]\n",
            " [6 2 5]\n",
            " [7 2 5]]\n",
            "Selected sample number:  5\n",
            "Actual output:  [ 62.  63. 103.]\n",
            "True class:  0\n",
            "SVM Loss:  44.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "def calculate_output(input_vector, w):\n",
        "    y = np.dot(input_vector,w)\n",
        "    print(\"Actual output: \",y)\n",
        "    return y\n",
        "def SVM_loss(y,true_class_index,delta=1):\n",
        "    \"\"\"\n",
        "    This function calculates the hinge loss function\n",
        "    Farhad Kamangar Apr. 2020\n",
        "\n",
        "    \"\"\"\n",
        "    print(\"True class: \",true_class_index)\n",
        "    margins = np.maximum(0, y - y[true_class_index] + delta)\n",
        "    margins[true_class_index] = 0\n",
        "    loss_i = np.sum(margins)\n",
        "    return loss_i\n",
        "\n",
        "x = np.array([[1.0, 1.0, 1,3], [1.0, 0,0,-3.5], [0,1,0,2],[5,1,2,3],[3,6,5,1],[3,7,7,1]])\n",
        "true_class_index=[1,2,0,1,0,1,2]\n",
        "w=np.array([[2,4,7], [1,5,6], [6,2,5], [7,2,5]])\n",
        "\n",
        "print(\"X= \",x)\n",
        "print(\"weights: \",w)\n",
        "selected_sample_number=5\n",
        "print(\"Selected sample number: \",selected_sample_number)\n",
        "y=calculate_output(x[selected_sample_number], w)\n",
        "\n",
        "# y=np.array([30,10,40])\n",
        "loss=SVM_loss(y,0,delta=1)\n",
        "# loss=SVM_loss(y,true_class_index[selected_sample_number],delta=1)\n",
        "print(\"SVM Loss: \", loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmk-p_Ykdq1U"
      },
      "source": [
        "## Cross Entropy Loss\n",
        "\n",
        "The cross entropy loss uses a softmax function to calculate the loss.\n",
        "\n",
        "### Softmax Function\n",
        "\n",
        "The softmax function gives a probabilistic interpretation to the output values and it is formulated as:\n",
        "\n",
        "$$\\large S(i)=\\frac{e^{y_i}}{\\sum_{j}^{C}e^{y_j}}$$\n",
        "\n",
        "where $\\large S(i)$ is the softmax value corresponding to the output $\\large y_i$, and $ C$ is the number of classes. This function interprets the outputs as unnormalized log probabilities of each class. Notice that the denominator of the above equation normalizes the probabilities so the total sums to 1.\n",
        "\n",
        "In other words the softmax function takes a vector of floating point numbers and proportionally compresses each number between zero and one such that the total adds up to 1.\n",
        "\n",
        "Using the softmax function, the cross entropy loss for sample $\\large s$ can be calculated as:\n",
        "\n",
        "$$\\large L_s=-log(\\frac{e^{y_{s_t}}}{\\sum_{j=1}^{C} e^{y_j}})$$\n",
        "\n",
        "\n",
        "where $\\large s$ is the sample number, $ C$ is the number of classes, and  $\\large s_t$ is the index of the true class for sample number $\\large s$.\n",
        "\n",
        "\n",
        "The above equation is really a simplified version of a discrete cross entropy between two distributions.\n",
        "\n",
        "Let's imagine that we have a true discrete distribution $p$ and an estimated discrete distribution $q$. The cross entropy between these two distribution is defined as:\n",
        "\n",
        "$$\\large H(p,q)= - \\sum_{x}p(x)log(q(x))$$\n",
        "\n",
        "Notice that in a multi-class classification problem the true probability distribution has all zeros except for the correct class, $i$, which has the value of 1:\n",
        "\n",
        "$$\\large p=[0,0,..., 1, ... 0]$$\n",
        "\n",
        "If the above discrete distribution $p$ is substituted into the general cross entropy equation it will result in the simplified cross entropy loss\n",
        "\n",
        "$$\\large L_s=-log(\\frac{e^{y_{s_t}}}{\\sum_{j=1}^{C} e^{y_j}})$$\n",
        "\n",
        "where $\\large s_t$ is the index of the correct class.\n",
        "\n",
        "Notice that to calculate the overall loss we still need to average the loss over all the samples.\n",
        "$$\\large L=\\frac {1}{N} \\sum_{s} L_s$$\n",
        "\n",
        "\n",
        "where $ N$  is the number of samples and $L_s$ is the cross entropy loss for sample $s$\n",
        "\n",
        "**Note:** There is no \"softmax loss\". The correct terminology is \"cross-entropy loss\". The \"cross entropy loss\" uses the \"softmax\" function to calculate the loss.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBFS2WNsdq1U"
      },
      "source": [
        "## Numerical Example\n",
        "Consider a one layer neural network with 3 nodes and linear activation function.\n",
        "This network is supposed to classify its input into one of three possible classes.\n",
        "Assume that the input to this network is 4 dimensional and the loss function is defined as cross entropy categorical.\n",
        "For simplicity assume that his network does not have any bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Q2Pjf9Ldq1U",
        "outputId": "892f3399-2e26-4815-bd6c-626f201806d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X=  [[ 1.   1.   1.   3. ]\n",
            " [ 1.   0.   0.  -3.5]\n",
            " [ 0.   1.   0.   2. ]\n",
            " [ 5.   1.   2.   3. ]\n",
            " [ 3.   6.   5.   1. ]\n",
            " [ 3.   7.   7.   1. ]]\n",
            "weights:  [[2 4 7]\n",
            " [1 5 6]\n",
            " [6 2 5]\n",
            " [7 2 5]]\n",
            "Selected sample number:  4\n",
            "Actual output:  [49. 54. 87.]\n",
            "Softmax:  [0.37275463 0.12407924 0.50316613]\n",
            "Cross entropy loss:  0.9868348922324128\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def calculate_output(input_vector, w):\n",
        "    y = np.dot(input_vector,w)\n",
        "    print(\"Actual output: \",y)\n",
        "    return y\n",
        "\n",
        "def cross_entropy_categorical_loss(y,true_class_index):\n",
        "    \"\"\"\n",
        "    This function calculates the categorical cross entropy\n",
        "    Farhad Kamangar Apr. 2020\n",
        "\n",
        "    \"\"\"\n",
        "#     y=np.exp(y - np.max(y)) # This is for numerical stability\n",
        "    softmax=np.exp(y) /np.sum(np.exp(y))\n",
        "    print(\"Softmax: \",softmax)\n",
        "    return -np.log(softmax[true_class_index])\n",
        "\n",
        "x = np.array([[1.0, 1.0, 1,3], [1.0, 0,0,-3.5], [0,1,0,2],[5,1,2,3],[3,6,5,1],[3,7,7,1]])\n",
        "true_class_index=[1,2,0,1,0,1]\n",
        "w=np.array([[2,4,7], [1,5,6], [6,2,5], [7,2,5]])\n",
        "\n",
        "print(\"X= \",x)\n",
        "print(\"weights: \",w)\n",
        "selected_sample_number=4\n",
        "print(\"Selected sample number: \",selected_sample_number)\n",
        "y=calculate_output(x[selected_sample_number], w)\n",
        "# y=np.array([1,-0.1,1.3])\n",
        "loss=cross_entropy_categorical_loss(y,0)\n",
        "# loss=cross_entropy_categorical_loss(y,true_class_index[selected_sample_number])\n",
        "print(\"Cross entropy loss: \", loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0avQQb1edq1U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}