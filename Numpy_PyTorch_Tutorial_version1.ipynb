{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a396da07",
      "metadata": {
        "id": "a396da07"
      },
      "source": [
        "# Single Neuron → Multiple Neurons → Weight Matrix Form (NumPy + PyTorch)\n",
        "\n",
        "This notebook is designed to build intuition step-by-step:\n",
        "\n",
        "1. **Single neuron** (dot product + bias)\n",
        "2. **Multiple neurons** (several neurons in a layer)\n",
        "3. **Matrix form**\n",
        "4. Implemented in **PyTorch tensors**\n",
        "\n",
        "## Notations\n",
        "\n",
        "We will use the following symbols and shapes throughout.\n",
        "\n",
        "| Symbol | Meaning | Shape |\n",
        "|---|---|---|\n",
        "| $x$ | input vector | $x \\in \\mathbb{R}^{d}$ |\n",
        "| $w$ | weights for **one neuron** | $w \\in \\mathbb{R}^{d}$ |\n",
        "| $b$ | bias for **one neuron** | $b \\in \\mathbb{R}$ |\n",
        "| $net$ | pre-activation (net input) | $net \\in \\mathbb{R}$ |\n",
        "| $a$ | activation function | elementwise |\n",
        "\n",
        "### Single neuron\n",
        "\n",
        "$$\n",
        "net =  x w^\\top+ b\n",
        "$$\n",
        "\n",
        "$$\n",
        "a = \\phi(net)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### A layer with $m$ neurons\n",
        "\n",
        "| Symbol | Meaning | Shape |\n",
        "|---|---|---|\n",
        "| $W$ | weight matrix (column $i$ is neuron $i$’s weights) | $W \\in \\mathbb{R}^{d \\times m}$ |\n",
        "| $b$ | bias vector | $b \\in  \\mathbb{R}^{m}$ |\n",
        "| $net$ | net vector/value | $net \\in  \\mathbb{R}^{m}$ |\n",
        "| $a$ | activation/output vector | $a \\in  \\mathbb{R}^{m}$ |\n",
        "\n",
        "Forward pass:\n",
        "\n",
        "$$\n",
        "net = xW + b\n",
        "$$\n",
        "\n",
        "$$\n",
        "a = \\phi(net)\n",
        "$$\n",
        "\n",
        "As of now we assume there is no activation function, so our net value is the output vector/value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7302427",
      "metadata": {
        "id": "d7302427"
      },
      "source": [
        "## Part A — NumPy\n",
        "We start with NumPy so you can clearly see shapes, dot products, and matrix multiplication."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc227a79",
      "metadata": {
        "id": "cc227a79"
      },
      "source": [
        "### 1) Single neuron\n",
        "\n",
        "A single neuron computes:\n",
        "\n",
        "$$\n",
        "net = \\mathbf{x}\\mathbf{w}^\\top + b\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "40ffd374",
      "metadata": {
        "id": "40ffd374",
        "outputId": "0adc2c08-d5c7-4303-9ef7-629d434bbcc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [ 0.47143516 -1.19097569  1.43270697 -0.3126519  -0.72058873]\n",
            "x shape: (5,)\n",
            "w: [ 0.88716294  0.85958841 -0.6365235   0.01569637 -2.24268495]\n",
            "w shape: (5,)\n",
            "b: 1.150035724719818\n",
            "net =  x w^T + b: 1.2437209721766265\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Seed for reproducibility\n",
        "np.random.seed(1234)\n",
        "\n",
        "\n",
        "# Inputs: x ∈ R^d\n",
        "x = np.random.randn(5)\n",
        "print(\"x:\", x)\n",
        "print(\"x shape:\", x.shape)\n",
        "\n",
        "# Weights: w ∈ R^d, bias: b ∈ R\n",
        "w = np.random.randn(x.shape[0])\n",
        "b = np.random.randn()\n",
        "\n",
        "print(\"w:\", w)\n",
        "print(\"w shape:\", w.shape)\n",
        "print(\"b:\", b)\n",
        "\n",
        "# Pre-activation (net input)\n",
        "net = np.dot(x,w) + b\n",
        "print(\"net =  x w^T + b:\", net)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b551ce1",
      "metadata": {
        "id": "4b551ce1"
      },
      "source": [
        "### 2) Multiple neurons\n",
        "\n",
        "If we have $m$ neurons, each neuron $j$ has its own weight vector $\\mathbf{w}_j$ and bias $b_j$:\n",
        "\n",
        "\n",
        "$$\n",
        "net_j = \\mathbf{x}\\mathbf{w}_j^\\top + b_j\\quad\\text{for }j=1,2,\\dots,m\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "aaa9e134",
      "metadata": {
        "id": "aaa9e134",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc1d3de-9ab1-49e5-b9eb-e40368d4ca96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [ 0.47143516 -1.19097569]\n",
            "x shape: (2,)\n",
            "weights: [array([ 1.43270697, -0.3126519 ]), array([-0.72058873,  0.88716294]), array([ 0.85958841, -0.6365235 ])]\n",
            "weights shape: (3, 2)\n",
            "biases: [0.015696372114428918, -2.2426849541854055, 1.150035724719818]\n",
            "net: [ 1.06348563 -3.63898532  2.31335995]\n",
            "net shape: (3,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Seed for reproducibility\n",
        "np.random.seed(1234)\n",
        "\n",
        "# Inputs: x ∈ R^d\n",
        "x = np.random.randn(2)\n",
        "print(\"x:\", x)\n",
        "print(\"x shape:\", x.shape)\n",
        "\n",
        "m = 3  # number of neurons in the layer\n",
        "\n",
        "# Each neuron has its own weight vector and bias\n",
        "weights = [np.random.randn(x.shape[0]) for _ in range(m)]  # list of w_j\n",
        "biases  = [np.random.randn() for _ in range(m)]            # list of b_j\n",
        "\n",
        "print(\"weights:\", weights)\n",
        "print(\"weights shape:\", np.array(weights).shape)\n",
        "print(\"biases:\", biases)\n",
        "\n",
        "net = []\n",
        "for j in range(m):\n",
        "    net_j = np.dot(x, weights[j]) + biases[j]\n",
        "    net.append(net_j)\n",
        "\n",
        "net = np.array(net)  # shape: (m,)\n",
        "print(\"net:\", net)\n",
        "print(\"net shape:\", net.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b5898d",
      "metadata": {
        "id": "02b5898d"
      },
      "source": [
        "### 3) Multiple neurons — matrix form\n",
        "\n",
        "Instead of storing the weight vectors $(\\mathbf{w}_1,\\dots,\\mathbf{w}_m)$ separately, we stack them as **columns** in a matrix:\n",
        "\n",
        "$$\n",
        "\\mathbf{W}=\n",
        "\\begin{bmatrix}\n",
        "\\;|\\; & \\;|\\; &        & \\;|\\; \\\\\n",
        "\\mathbf{w}_1 & \\mathbf{w}_2 & \\cdots & \\mathbf{w}_m \\\\\n",
        "\\;|\\; & \\;|\\; &        & \\;|\\;\n",
        "\\end{bmatrix}\n",
        "\\in \\mathbb{R}^{d\\times m}\n",
        "$$\n",
        "\n",
        "We also stack the biases into a vector:\n",
        "\n",
        "$$\n",
        "\\mathbf{b}=\n",
        "\\begin{bmatrix}\n",
        "b_1\\\\\n",
        "b_2\\\\\n",
        "\\vdots\\\\\n",
        "b_m\n",
        "\\end{bmatrix}\n",
        "\\in \\mathbb{R}^{m}\n",
        "$$\n",
        "\n",
        "Then the layer output is:\n",
        "\n",
        "$$\n",
        "\\mathbf{net} = \\mathbf{x}^\\top \\mathbf{W} + \\mathbf{b}^\\top\n",
        "$$\n",
        "\n",
        "#### Bias trick (augment the input)\n",
        "\n",
        "We can absorb the bias into the matrix multiplication by appending a constant $1$ to the input:\n",
        "\n",
        "$$\n",
        "{\\mathbf{x}}=\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{1}\\\\\n",
        "x\n",
        "\\end{bmatrix}\n",
        "\\in \\mathbb{R}^{d+1}\n",
        "$$\n",
        "\n",
        "and appending the bias as an **extra row** under $\\mathbf{W}$:\n",
        "\n",
        "$$\n",
        "{\\mathbf{W}}=\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{W}\\\\\n",
        "\\mathbf{b}^\\top\n",
        "\\end{bmatrix}\n",
        "\\in \\mathbb{R}^{(d+1)\\times m}\n",
        "$$\n",
        "\n",
        "Now the same computation becomes a single matrix product:\n",
        "\n",
        "$$\n",
        "\\mathbf{net} = {\\mathbf{x}}{\\mathbf{W}}\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "31345838",
      "metadata": {
        "id": "31345838",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d17ff39-f50c-420f-9590-aa9e8d6f70ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Standard matrix form (xW + b) ===\n",
            "W shape: (2, 3)\n",
            "x shape: (2,)\n",
            "b shape: (3,)\n",
            "net: [ 1.06348563 -3.63898532  2.31335995]\n",
            "\n",
            "=== Bias trick ===\n",
            "x_tilde shape: (3,)\n",
            "W_tilde shape: (3, 3)\n",
            "net_bias_trick: [ 1.06348563 -3.63898532  2.31335995]\n",
            "\n",
            "Matches standard form? True\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "W = np.stack(weights, axis=1)   # shape: (d, m)  columns = neurons\n",
        "b = np.array(biases)            # shape: (m,)\n",
        "\n",
        "net = np.dot(x,W) + b                 # shape: (m,)\n",
        "\n",
        "print(\"=== Standard matrix form (xW + b) ===\")\n",
        "print(\"W shape:\", W.shape)      # (d, m)\n",
        "print(\"x shape:\", x.shape)      # (d,)\n",
        "print(\"b shape:\", b.shape)      # (m,)\n",
        "print(\"net:\", net)\n",
        "\n",
        "\n",
        "x_tilde = np.concatenate([x, np.array([1.0])])  # shape: (d+1,)\n",
        "W_tilde = np.concatenate([W, b.reshape(1, -1)], axis=0)  # shape: (d+1, m)\n",
        "\n",
        "net_bias_trick = np.dot(x_tilde,W_tilde)             # shape: (m,)\n",
        "\n",
        "print(\"\\n=== Bias trick ===\")\n",
        "print(\"x_tilde shape:\", x_tilde.shape)          # (d+1,)\n",
        "print(\"W_tilde shape:\", W_tilde.shape)          # (d+1, m)\n",
        "print(\"net_bias_trick:\", net_bias_trick)\n",
        "\n",
        "print(\"\\nMatches standard form?\", np.allclose(net, net_bias_trick))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8aed248",
      "metadata": {
        "id": "e8aed248"
      },
      "source": [
        "---\n",
        "## Part B — PyTorch\n",
        "\n",
        "PyTorch gives us fast tensor operations (like NumPy) and optional automatic differentiation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02859bbf",
      "metadata": {
        "id": "02859bbf"
      },
      "source": [
        "### 1) Single neuron (PyTorch)\n",
        "\n",
        "Same equation:\n",
        "$$\n",
        "net = \\mathbf{x}\\mathbf{w}^\\top + b\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f5438214",
      "metadata": {
        "id": "f5438214",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d08dbd-e47d-4243-a658-56efcd26005f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.9.0+cpu\n",
            "x: tensor([ 0.0461,  0.4024, -1.0115,  0.2167, -0.6123])\n",
            "x shape: (5,)\n",
            "w: tensor([ 0.5036,  0.2310,  0.6931, -0.2669,  2.1785])\n",
            "w shape: (5,)\n",
            "b: tensor(0.1021)\n",
            "b shape: ()\n",
            "net =  x w^T + b: tensor(-1.8744)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "\n",
        "\n",
        "# Inputs: x ∈ R^d\n",
        "x = torch.randn(5)                 # shape: (d,)\n",
        "print(\"x:\", x)\n",
        "print(\"x shape:\", tuple(x.shape))\n",
        "\n",
        "# Weights: w ∈ R^d, bias: b ∈ R\n",
        "w = torch.randn(x.shape[0])        # shape: (d,)\n",
        "b = torch.randn(())\n",
        "\n",
        "print(\"w:\", w)\n",
        "print(\"w shape:\", tuple(w.shape))\n",
        "print(\"b:\", b)\n",
        "print(\"b shape:\", tuple(b.shape))\n",
        "\n",
        "# Pre-activation (net input): net = w^T x + b\n",
        "net = torch.dot(x, w) + b          # scalar\n",
        "print(\"net =  x w^T + b:\", net)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ce3042",
      "metadata": {
        "id": "73ce3042"
      },
      "source": [
        "### 2) Multiple neurons (layer) — matrix form (PyTorch)\n",
        "\n",
        "If $\\mathbf{x}\\in\\mathbb{R}^{d}$, $\\mathbf{W}\\in\\mathbb{R}^{d\\times m}$, and $\\mathbf{b}\\in\\mathbb{R}^{m}$:\n",
        "\n",
        "$$\n",
        "\\mathbf{net}=\\mathbf{x}\\mathbf{W}+\\mathbf{b}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "32f5a8ff",
      "metadata": {
        "id": "32f5a8ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2edf0ef0-bc76-48b6-8843-8e3d6bafb3cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: tensor([0.0461, 0.4024])\n",
            "x shape: (2,)\n",
            "weights (list): [tensor([-1.0115,  0.2167]), tensor([-0.6123,  0.5036]), tensor([0.2310, 0.6931])]\n",
            "weights stacked shape: (2, 3)\n",
            "biases (list): [tensor(-0.2669), tensor(2.1785), tensor(0.1021)]\n",
            "net: tensor([-0.2263,  2.3529,  0.3917])\n",
            "net shape: (3,)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1234)\n",
        "\n",
        "# Inputs: x ∈ R^d\n",
        "x = torch.randn(2)\n",
        "print(\"x:\", x)\n",
        "print(\"x shape:\", tuple(x.shape))\n",
        "\n",
        "m = 3  # number of neurons\n",
        "\n",
        "# Each neuron has its own weight vector and bias\n",
        "weights = [torch.randn(x.shape[0]) for _ in range(m)]\n",
        "biases  = [torch.randn(()) for _ in range(m)]\n",
        "\n",
        "print(\"weights (list):\", weights)\n",
        "print(\"weights stacked shape:\", tuple(torch.stack(weights, dim=1).shape))  # (d, m)\n",
        "print(\"biases (list):\", biases)\n",
        "\n",
        "net_list = []\n",
        "for j in range(m):\n",
        "    net_j = torch.dot(x,weights[j]) + biases[j]\n",
        "    net_list.append(net_j)\n",
        "\n",
        "net = torch.stack(net_list)   # shape: (m,)\n",
        "print(\"net:\", net)\n",
        "print(\"net shape:\", tuple(net.shape))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"3) Matrix form\")\n",
        "\n",
        "W = torch.stack(weights,dim=1)    # shape: (d, m)\n",
        "b_vec = torch.stack(biases)        # shape: (m,)\n",
        "\n",
        "# net = x W  + b\n",
        "net_mat = torch.matmul(x,W) + b_vec\n",
        "\n",
        "print(\"W shape:\", tuple(W.shape))          # (d, m)\n",
        "print(\"x shape:\", tuple(x.shape))          # (d,)\n",
        "print(\"b_vec shape:\", tuple(b_vec.shape)) # (m,)\n",
        "print(\"net_mat:\", net_mat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh6bn11O8JSw",
        "outputId": "c693882d-6283-472d-8909-5af038593cd3"
      },
      "id": "Vh6bn11O8JSw",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3) Matrix form\n",
            "W shape: (2, 3)\n",
            "x shape: (2,)\n",
            "b_vec shape: (3,)\n",
            "net_mat: tensor([-0.2263,  2.3529,  0.3917])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"4) Bias trick\")\n",
        "\n",
        "# x_tilde = [x; 1]\n",
        "x_tilde = torch.cat([x, torch.tensor([1.0])])           # shape: (d+1,)\n",
        "\n",
        "# W_tilde = [W | b] (append b as last column)\n",
        "W_tilde = torch.cat([W, b_vec.unsqueeze(0)], dim=0)     # shape: (d+1, m)\n",
        "\n",
        "net_bias_trick = torch.matmul(x_tilde,W_tilde)                      # shape: (m,)\n",
        "\n",
        "print(\"x_tilde shape:\", tuple(x_tilde.shape))           # (d+1,)\n",
        "print(\"W_tilde shape:\", tuple(W_tilde.shape))           # (d+1, m)\n",
        "print(\"net_bias_trick:\", net_bias_trick)\n",
        "\n",
        "print(\"Matches matrix form?\", torch.allclose(net_bias_trick, net_mat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPap350r8Weg",
        "outputId": "83996081-f234-4608-c5d4-fb59ea09c5fc"
      },
      "id": "hPap350r8Weg",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4) Bias trick\n",
            "x_tilde shape: (3,)\n",
            "W_tilde shape: (3, 3)\n",
            "net_bias_trick: tensor([-0.2263,  2.3529,  0.3917])\n",
            "Matches matrix form? True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XZ0llkjK87Id"
      },
      "id": "XZ0llkjK87Id",
      "execution_count": 13,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}